{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Innovating AI-Driven Marketing Solutions\n",
        "\n",
        "Hello, I'm Sanchay Thalnerkar, an AI Engineer with a relentless drive to explore the frontiers of artificial intelligence. This notebook is a culmination of cutting-edge AI techniques tailored to revolutionize the marketing and social media landscapes.\n",
        "\n",
        "In this collaborative workspace, you'll find tools and methods that harness the power of AI to generate innovative marketing scenarios, fine-tune advanced language models, and ensure diversity in AI-generated content. Whether you're here to learn, contribute, or simply explore, this notebook is designed to inspire creativity and push the boundaries of what's possible with AI in marketing.\n",
        "\n",
        "Let's dive in and create something extraordinary together."
      ],
      "metadata": {
        "id": "FSVldL-7AXTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List , Dict\n",
        "from together import Together\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from ratelimit import limits , sleep_and_retry\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "gUdalThZpr-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Together API Key from Google Colab userdata\n",
        "\n",
        "### The API key is securely loaded from the Colab environment to ensure proper access to the Together API services."
      ],
      "metadata": {
        "id": "7VA8IWGV-OcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "TOGETHER_API_KEY = userdata.get('together')\n"
      ],
      "metadata": {
        "id": "pnRnvPP0qdYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the Together API Client\n",
        "\n",
        "### Setting up the Together API client with the retrieved API key.\n",
        "### A rate-limited function is defined to handle API calls, ensuring that calls are within the allowed limits.\n",
        "### The function uses the Meta LLaMA model for generating completions."
      ],
      "metadata": {
        "id": "woR4rCjf-T8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "together = Together(api_key = TOGETHER_API_KEY )\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls = 1,period = 1)\n",
        "def rate_limited_api_call(messages):\n",
        "    return together.chat.completions.create(\n",
        "        model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "qQkSDDITpsZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response  = rate_limited_api_call(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "RqQn73G_pscF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "console = Console()"
      ],
      "metadata": {
        "id": "yWvWUoD_xbvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted = Markdown(response.choices[0].message.content.strip())"
      ],
      "metadata": {
        "id": "gbB59trzpseT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executing an API call to generate a response\n",
        "\n",
        "# Using the defined `rate_limited_api_call` function to send a request to the Together model and store the response."
      ],
      "metadata": {
        "id": "tbUffVQx-r8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_responses(messages : List[Dict[str,str]] , max_retries : int = 3 , timeout : int=60 ) -> str:\n",
        "  for attempt in range(max_retries):\n",
        "    try:\n",
        "      respone = rate_limited_api_call(messages)\n",
        "      return Markdown(response.choices[0].message.content.strip())\n",
        "    except Exception as e:\n",
        "      console.print(f\"[bold red] Error in the model response ( Attempt { attempt + 1} / {max_retries} ) : {e}  [/bold red]\")\n",
        "      if attempt < max_retries -1 :\n",
        "        time.sleep(5)\n",
        "  raise Exception( \" Failed to get the response after repeated errors\")\n"
      ],
      "metadata": {
        "id": "2UpVGCEdpsgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Multiple Marketing Scenarios\n",
        "\n",
        "This function, `generate_multiple_marketing_samples`, creates a set number of diverse and innovative marketing scenarios.\n",
        "Each scenario is formatted with an instruction, input, and response, making it suitable for fine-tuning AI models for marketing tasks."
      ],
      "metadata": {
        "id": "Qa5IIFrD-5Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_multiple_marketing_samples(num_samples: int = 5) -> List[str]:\n",
        "    system_message = f\"\"\"You are an elite marketing AI, tasked with generating {num_samples} exceptional, diverse, and innovative marketing scenarios. Each scenario must be unique and push the boundaries of creative marketing. Your goal is to create a dataset that will revolutionize AI-assisted marketing when used for fine-tuning.\n",
        "\n",
        "    For each sample, provide:\n",
        "\n",
        "    1. Instruction: A specific, challenging marketing task that requires creativity and expertise.\n",
        "    2. Input: Detailed context, including company info, target audience, constraints, and goals.\n",
        "    3. Response: An outstanding, creative solution that addresses the instruction and input.\n",
        "\n",
        "    Guidelines:\n",
        "    - Cover a wide range of industries: tech, fashion, food, finance, entertainment, healthcare, education, travel, etc.\n",
        "    - Include various marketing channels: social media, email, content marketing, influencer partnerships, guerrilla marketing, AR/VR campaigns, etc.\n",
        "    - Address different marketing objectives: brand awareness, lead generation, customer retention, product launch, crisis management, etc.\n",
        "    - Incorporate current trends: sustainability, AI, personalization, interactive content, social causes, etc.\n",
        "    - Consider various audience segments: Gen Z, millennials, seniors, B2B executives, niche hobbyists, global markets, etc.\n",
        "    - Include challenging scenarios: limited budgets, tight deadlines, highly competitive markets, rebranding efforts, etc.\n",
        "    - Explore innovative formats: interactive stories, user-generated content campaigns, multi-platform narratives, gamification, etc.\n",
        "\n",
        "    Format each sample as follows:\n",
        "    ### Instruction:\n",
        "    [Concise, specific marketing task]\n",
        "\n",
        "    ### Input:\n",
        "    [Detailed context and requirements]\n",
        "\n",
        "    ### Response:\n",
        "    [Creative, comprehensive marketing solution]\n",
        "\n",
        "    Separate each sample with ---\n",
        "\n",
        "    Remember: Quality, creativity, and diversity are crucial. Each sample should be a masterclass in innovative marketing that would challenge and inspire even seasoned professionals.\"\"\"\n",
        "\n",
        "    user_message = f\"Generate {num_samples} cutting-edge, diverse marketing scenarios that showcase the future of AI-assisted marketing creativity. Use the specified format, separating each sample with ---.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ]\n",
        "\n",
        "    response = get_model_response(messages)\n",
        "    return [sample.strip() for sample in response.split(\"---\") if sample.strip()]\n"
      ],
      "metadata": {
        "id": "sItV1ZMwpspV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quality Check for Generated Samples\n",
        "\n",
        "The `quality_check` function ensures that each generated marketing scenario meets specific quality standards.\n",
        "It checks for minimum length and word diversity to maintain the dataset's overall quality."
      ],
      "metadata": {
        "id": "flZ0qM93_t-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quality_check(sample: dict) -> bool:\n",
        "    min_length = 50\n",
        "    max_repetition = 0.3\n",
        "\n",
        "    for key in [\"instruction\", \"input\", \"response\"]:\n",
        "        text = sample[key]\n",
        "        if len(text) < min_length:\n",
        "            return False\n",
        "\n",
        "        words = re.findall(r\"\\w+\", text.lower())\n",
        "        unique_words = set(words)\n",
        "        if len(unique_words) / len(words) < (1 - max_repetition):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n"
      ],
      "metadata": {
        "id": "sHgPK_Szpsrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracking Diversity Across Scenarios\n",
        "\n",
        "The `track_diversity` function analyzes the diversity of the generated marketing samples by tracking key industry, channel, and objective keywords.\n",
        "It provides a summary of the dataset's diversity, helping to ensure a wide range of marketing strategies are represented."
      ],
      "metadata": {
        "id": "Ol4Szuo9_vQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "industry_keywords = [\n",
        "    \"tech\",\n",
        "    \"fashion\",\n",
        "    \"food\",\n",
        "    \"finance\",\n",
        "    \"entertainment\",\n",
        "    \"healthcare\",\n",
        "    \"education\",\n",
        "    \"travel\",\n",
        "]\n",
        "channel_keywords = [\n",
        "    \"social media\",\n",
        "    \"email\",\n",
        "    \"content marketing\",\n",
        "    \"influencer\",\n",
        "    \"guerrilla\",\n",
        "    \"AR\",\n",
        "    \"VR\",\n",
        "]\n",
        "objective_keywords = [\n",
        "    \"brand awareness\",\n",
        "    \"lead generation\",\n",
        "    \"customer retention\",\n",
        "    \"product launch\",\n",
        "    \"crisis management\",\n",
        "]\n",
        "\n",
        "\n",
        "def track_diversity(dataset: List[dict]) -> None:\n",
        "    industries = Counter()\n",
        "    channels = Counter()\n",
        "    objectives = Counter()\n",
        "\n",
        "    for sample in dataset:\n",
        "        text = \" \".join(\n",
        "            [sample[\"instruction\"], sample[\"input\"], sample[\"response\"]]\n",
        "        ).lower()\n",
        "        industries.update(word for word in industry_keywords if word in text)\n",
        "        channels.update(word for word in channel_keywords if word in text)\n",
        "        objectives.update(word for word in objective_keywords if word in text)\n",
        "\n",
        "    console.print(\"[bold]Dataset Diversity:\")\n",
        "    console.print(f\"Industries: {dict(industries)}\")\n",
        "    console.print(f\"Channels: {dict(channels)}\")\n",
        "    console.print(f\"Objectives: {dict(objectives)}\")\n"
      ],
      "metadata": {
        "id": "3g7Z3Wz77qGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Fine-Tuning Dataset for Marketing & Social Media Content\n",
        "\n",
        "This function, `create_finetuning_dataset`, is responsible for generating a large dataset for fine-tuning AI models specifically for marketing and social media content.\n",
        "The process involves making multiple API calls to generate samples, ensuring each sample meets quality standards before being added to the dataset.\n",
        "The progress is continuously saved, and diversity across different marketing scenarios is tracked and reported."
      ],
      "metadata": {
        "id": "tahsVh9A_6eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_finetuning_dataset(target_samples: int, output_file: str):\n",
        "    console.print(\n",
        "        Panel(\n",
        "            f\"Creating fine-tuning dataset for [bold]Marketing & Social Media Content[/bold]\",\n",
        "            expand=False,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    dataset = []\n",
        "    samples_per_call = 50  # Reduced to respect token limits\n",
        "    calls_made = 0\n",
        "    max_calls = 200  # Increased to allow for more total samples\n",
        "\n",
        "    with console.status(\"[bold green]Generating samples...\") as status:\n",
        "        while len(dataset) < target_samples and calls_made < max_calls:\n",
        "            calls_made += 1\n",
        "            status.update(\n",
        "                f\"API call {calls_made} (Dataset size: {len(dataset)}/{target_samples})\"\n",
        "            )\n",
        "\n",
        "            samples = generate_multiple_marketing_samples(samples_per_call)\n",
        "\n",
        "            for sample in samples:\n",
        "                if len(dataset) >= target_samples:\n",
        "                    break\n",
        "\n",
        "                parts = sample.split(\"###\")\n",
        "                if len(parts) == 4:\n",
        "                    instruction = parts[1].replace(\"Instruction:\", \"\").strip()\n",
        "                    input_text = parts[2].replace(\"Input:\", \"\").strip()\n",
        "                    response = parts[3].replace(\"Response:\", \"\").strip()\n",
        "\n",
        "                    sample_dict = {\n",
        "                        \"instruction\": instruction,\n",
        "                        \"input\": input_text,\n",
        "                        \"response\": response,\n",
        "                    }\n",
        "\n",
        "                    if quality_check(sample_dict):\n",
        "                        dataset.append(sample_dict)\n",
        "                    else:\n",
        "                        console.print(\"[yellow]Skipped low-quality sample\")\n",
        "                else:\n",
        "                    console.print(\n",
        "                        f\"[yellow]Skipped malformed sample: {sample[:100]}...\"\n",
        "                    )\n",
        "\n",
        "            # Save progress after each API call\n",
        "            with open(output_file, \"w\") as f:\n",
        "                json.dump(dataset, f, indent=2)\n",
        "\n",
        "    console.print(\n",
        "        f\"[bold green]Dataset creation complete! Total samples: {len(dataset)}\"\n",
        "    )\n",
        "    track_diversity(dataset)\n"
      ],
      "metadata": {
        "id": "iY3HM85T7wzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executing the Fine-Tuning Dataset Creation Process\n",
        "\n",
        "This cell executes the `create_finetuning_dataset` function to generate 1000 samples and saves the results in a JSON file named \"marketing_social_media_dataset_v1.json\"."
      ],
      "metadata": {
        "id": "2EEUwkLz_8Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_finetuning_dataset(\n",
        "        target_samples=1000,\n",
        "        output_file=\"marketing_social_media_dataset_v1.json\",\n",
        "    )\n"
      ],
      "metadata": {
        "id": "nYrvSfTx7zfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a Pre-Trained 4-bit Quantized Language Model\n",
        "\n",
        "This cell demonstrates the process of loading a pre-trained language model using the `FastLanguageModel` class from the `unsloth` library.\n",
        "The model is set up with specific configurations, such as a sequence length of 600 and 4-bit quantization, to optimize performance and reduce memory usage.\n",
        "A variety of 4-bit quantized models are supported, allowing for faster downloads and avoiding out-of-memory issues.\n",
        "In this example, the \"Meta-Llama-3.1-8B\" model is loaded, which is suitable for tasks requiring extensive token processing."
      ],
      "metadata": {
        "id": "Fg70m_eXAQdN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 600 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "## Loading a Pre-Trained 4-bit Quantized Language Model\n",
        "\n",
        "This cell demonstrates how to load a pre-trained language model using the `FastLanguageModel` class from the `unsloth` library.\n",
        "The model is loaded with 4-bit quantization to reduce memory usage, making it efficient to work with on limited hardware.\n",
        "A list of supported 4-bit models is provided, and the specific model \"Meta-Llama-3.1-8B\" is loaded for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"response\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('json', data_files='/content/marketing_social_media_dataset_v1.json', split='train')\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Create a marketing campaign to promote the choclate bar\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CrRnSVL7pqw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Write a marketing campaign for zara.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Create a marketing campaign to promote the choclate bar\", # instruction\n",
        "        \"Company : Cadbury , target audience : adults/boomers\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}